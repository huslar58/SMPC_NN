{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plaintext classification neural network model\n",
    "\n",
    "In the following code, the plaintext neural network model is implemented. Before the useage of the model, the data is cleaned based on the code from the ADML module at Hochschule Luzern by Solange Emmenegger (Solange Emmenegger, Hochschule Luzern, Module Advanced Machine Learning, accessed on 19 April 2024 at https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/tree/master/notebooks/03A%20Supervised%20Learning, and https://gitlab.renku.hslu.ch/solange.emmenegger/ml-adml-hslu/-/blob/master/notebooks/04B%20Gradient%20Descent/Gradient%20Descent.ipynb) and modified where necessary. \n",
    "\n",
    "The model architecture was build by the authour with assistance from the Claude, ChatGPT and Gemini Chatbots. (“Please suggest a PyTorch neural network architecture for binary classification”, Claude 3, Anthropic PBC, generated on 27 March 2024., “Please improve this neural network architecture for binary classification…”, ChatGPT (GPT-4), OpenAI, generated on 27 March 2024., “Why is the F2 score of the crypten model worse and how do I improve it”, Gemini, Google (Alphabet Inc.), generated on 30 March 2024. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from time import time\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and preparation of the dataset\n",
    "\n",
    "In the code below, the dataset is split into a training and test set, it is scaled and trimmed to 10000 samples, which is the size used for the evaluation. This size was chosen to avoid the model training for days, as the author of the project has constrained hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(855, 8)\n",
      "X_train_transactions has shape: (800000, 7)\n",
      "y_train_transactions has shape: (800000,)\n",
      "X_test_transactions has shape: (200000, 7)\n",
      "y_test_transactions has shape: (200000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87107/423273962.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  print(train_transactions[:10000][train_transactions['fraud'] == 1.0].shape)\n"
     ]
    }
   ],
   "source": [
    "df_nn = pd.read_csv(\"card_transdata.csv\")\n",
    "\n",
    "train_transactions, test_transactions = train_test_split(df_nn, test_size=0.2, random_state=42)\n",
    "print(train_transactions[:10000][train_transactions['fraud'] == 1.0].shape)\n",
    "\n",
    "X_train_transactions = train_transactions.drop(columns=[\"fraud\"])\n",
    "y_train_transactions = train_transactions.fraud.values\n",
    "X_test_transactions = test_transactions.drop(columns=[\"fraud\"])\n",
    "y_test_transactions = test_transactions.fraud.values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_transactions = pd.DataFrame(scaler.fit_transform(X_train_transactions), columns=X_train_transactions.columns, index=X_train_transactions.index).values\n",
    "X_test_transactions = pd.DataFrame(scaler.transform(X_test_transactions), columns=X_test_transactions.columns, index=X_test_transactions.index).values\n",
    "\n",
    "print(f\"X_train_transactions has shape: {X_train_transactions.shape}\")\n",
    "print(f\"y_train_transactions has shape: {y_train_transactions.shape}\")\n",
    "print(f\"X_test_transactions has shape: {X_test_transactions.shape}\")\n",
    "print(f\"y_test_transactions has shape: {y_test_transactions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture\n",
    "\n",
    "Below is the neural network architecture, as described in the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetectionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, lr=0.001):\n",
    "        super(FraudDetectionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer to prevent overfitting\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size, dtype=torch.float64)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes, dtype=torch.float64)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes, dtype=torch.float64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model hyperparameters, cost function and the learning rate\n",
    "\n",
    "Below we select the device used for model training. If a NVIDIA GPU is available, the GPU is used, otherwise the CPU is used.\n",
    "We have the input size of 7 as we have seven features. The hidden size of 64 is chosen, which is a hyperparameter and is chosen heuristically by experimentation. \n",
    "Number of classes is set to 1, which indicates binary classification. \n",
    "\n",
    "We train the model over 1000 epochs, which was chosen for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Initialize model parameters\n",
    "input_size = 7  # Number of input features\n",
    "hidden_size = 64  # Number of hidden units\n",
    "num_classes = 1  # Binary classification\n",
    "num_epochs = 5\n",
    "\n",
    "model = FraudDetectionModel(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function and training\n",
    "\n",
    "Below the training function and its execution is defined, which is the usual format for training a pytorch neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, criterion, x, y, epochs=num_epochs):\n",
    "    batch_size = 32 \n",
    "    num_batches = x.size(0) // batch_size\n",
    "\n",
    "    for e in tqdm(range(1, epochs + 1)):\n",
    "            \n",
    "        for batch in range(num_batches):\n",
    "            # define the start and end of the training mini-batch\n",
    "            start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "            x_train = x[start:end]\n",
    "            y_train = y[start:end]\n",
    "            optim.zero_grad()\n",
    "            out = model(x_train.to(device))\n",
    "            loss = criterion(out, y_train.to(device))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        accuracy = accuracy_score(y_train.detach().numpy(),  np.where(out.detach().numpy() > 0.5, 1, 0))\n",
    "        f2_score = fbeta_score(y_train.detach().numpy(), np.where(out.detach().numpy() > 0.5, 1, 0), beta=0.5)\n",
    "        print(f\"Loss at epoch {e}: {loss.data}\")\n",
    "        print(f\"Accuracy at epoch {e}: {accuracy}\")\n",
    "        print(f\"F2 score at epoch {e}: {f2_score}\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfba2e6a97645da90cefea597542bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.21605794707204967\n",
      "Accuracy at epoch 1: 1.0\n",
      "F2 score at epoch 1: 1.0\n",
      "Loss at epoch 2: 0.12521208433681993\n",
      "Accuracy at epoch 2: 0.96875\n",
      "F2 score at epoch 2: 0.7142857142857143\n",
      "Loss at epoch 3: 0.04226166487500828\n",
      "Accuracy at epoch 3: 1.0\n",
      "F2 score at epoch 3: 1.0\n",
      "Loss at epoch 4: 0.03752758370388637\n",
      "Accuracy at epoch 4: 1.0\n",
      "F2 score at epoch 4: 1.0\n",
      "Loss at epoch 5: 0.09614952146572958\n",
      "Accuracy at epoch 5: 0.96875\n",
      "F2 score at epoch 5: 0.8333333333333334\n",
      "Training of the neural network took 7 seconds\n",
      "Memory usage difference: 11341824 bytes\n"
     ]
    }
   ],
   "source": [
    "len_size = 50000\n",
    "\n",
    "# measure time\n",
    "t_start = time()\n",
    "# measure resource usage\n",
    "mem_usage = psutil.Process().memory_info().rss\n",
    "model = train(model, optimizer, criterion, torch.from_numpy(X_train_transactions[:len_size]), torch.from_numpy(y_train_transactions[:len_size]).view(-1, 1))\n",
    "mem_usage_end = psutil.Process().memory_info().rss\n",
    "\n",
    "# Calculate the differences\n",
    "mem_diff = mem_usage_end - mem_usage\n",
    "t_end = time()\n",
    "print(f\"Training of the neural network took {int(t_end - t_start)} seconds\")\n",
    "print(f\"Memory usage difference: {mem_diff} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model is evaluated on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9856\n",
      "F2 score: 0.9479582146248813\n"
     ]
    }
   ],
   "source": [
    "y_pred = model(torch.from_numpy(X_test_transactions).to(device))\n",
    "y_pred = np.where(y_pred.detach().numpy() > 0.5, 1, 0)\n",
    "print(\"Accuracy score: {}\".format(accuracy_score(y_test_transactions, y_pred)))\n",
    "print(\"F2 score: {}\".format(fbeta_score(y_test_transactions, y_pred=y_pred, beta=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the helper function for visualisation of the accuracy and F2 scores is defined and the scores are visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     plot_validation_curve(hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m], ax[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m---> 26\u001b[0m plot_validation_curves_acc(\u001b[43mhist\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_validation_curve(data, ax=None, ylim=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\"Validation Curve\")\n",
    "        ax.set_ylabel(\"Cost\")\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.plot(data)\n",
    "\n",
    "\n",
    "def plot_validation_curves_acc(hist, ylim=None):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "\n",
    "    ax[0].set_title(\"Train Cost\")\n",
    "    ax[0].set_ylabel(\"Cost\")\n",
    "    plot_validation_curve(hist[\"train_cost\"], ax[0], ylim)\n",
    "\n",
    "    ax[1].set_title(\"Train accuracy\")\n",
    "    ax[1].set_ylabel(\"F2\")\n",
    "    ax[1].set_ylim(-1, 1)\n",
    "    plot_validation_curve(hist[\"train_acc\"], ax[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_validation_curves_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_curves_f2(hist, ylim=None):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "\n",
    "    ax[0].set_title(\"Train Cost\")\n",
    "    ax[0].set_ylabel(\"Cost\")\n",
    "    plot_validation_curve(hist[\"train_cost\"], ax[0], ylim)\n",
    "\n",
    "    ax[1].set_title(\"Train F2\")\n",
    "    ax[1].set_ylabel(\"F2\")\n",
    "    ax[1].set_ylim(-1, 1)\n",
    "    plot_validation_curve(hist[\"train_f2\"], ax[1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_validation_curves_f2(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
